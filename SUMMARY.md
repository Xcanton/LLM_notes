# Table of contents

* [\_index](README.md)
* [ðŸŒŒ before-llm-era](before-llm-era/README.md)
  * [\_index](before-llm-era/\_index.md)
  * [Pre-trained Models](before-llm-era/pre-trained-models/README.md)
    * [\_index](before-llm-era/pre-trained-models/\_index.md)
    * [Bert](before-llm-era/pre-trained-models/bert.md)
    * [Transformer](before-llm-era/pre-trained-models/transformer.md)
    * [Attention](before-llm-era/pre-trained-models/attention/README.md)
      * [\_index](before-llm-era/pre-trained-models/attention/\_index.md)
      * [Multi-head Attention](before-llm-era/pre-trained-models/attention/multi-head-attention.md)
      * [Self-Attention](before-llm-era/pre-trained-models/attention/self-attention.md)
* [ðŸŒ… the-dawn-of-llm-era](the-dawn-of-llm-era/README.md)
  * [\_index](the-dawn-of-llm-era/\_index.md)
  * [Bloom](the-dawn-of-llm-era/bloom.md)
  * [Other LLMs](the-dawn-of-llm-era/other-llms.md)
  * [T5](the-dawn-of-llm-era/t5.md)
  * [Baichuan](the-dawn-of-llm-era/baichuan/README.md)
    * [\_index](the-dawn-of-llm-era/baichuan/\_index.md)
    * [Baichuan-v2](the-dawn-of-llm-era/baichuan/baichuan-v2.md)
  * [ChatGLM](the-dawn-of-llm-era/chatglm/README.md)
    * [\_index](the-dawn-of-llm-era/chatglm/\_index.md)
    * [ChatGLM-v2](the-dawn-of-llm-era/chatglm/chatglm-v2.md)
    * [ChatGLM-v3](the-dawn-of-llm-era/chatglm/chatglm-v3.md)
    * [GLM](the-dawn-of-llm-era/chatglm/glm.md)
  * [GPT Series](the-dawn-of-llm-era/gpt-series/README.md)
    * [\_index](the-dawn-of-llm-era/gpt-series/\_index.md)
    * [gpt](the-dawn-of-llm-era/gpt-series/gpt.md)
    * [GPT2](the-dawn-of-llm-era/gpt-series/gpt2.md)
    * [Instruction-GPT](the-dawn-of-llm-era/gpt-series/instruction-gpt.md)
    * [GPT4](the-dawn-of-llm-era/gpt-series/gpt4/README.md)
      * [\_index](the-dawn-of-llm-era/gpt-series/gpt4/\_index.md)
      * [GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE](the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe.md)
  * [LLaMa Series](the-dawn-of-llm-era/llama-series/README.md)
    * [\_index](the-dawn-of-llm-era/llama-series/\_index.md)
    * [LLaMa](the-dawn-of-llm-era/llama-series/llama.md)
  * [P5](the-dawn-of-llm-era/p5/README.md)
    * [\_index](the-dawn-of-llm-era/p5/\_index.md)
    * [How to Index Item IDs](the-dawn-of-llm-era/p5/how-to-index-item-ids.md)
  * [Qwen](the-dawn-of-llm-era/qwen/README.md)
    * [\_index](the-dawn-of-llm-era/qwen/\_index.md)
    * [Qwen-v2](the-dawn-of-llm-era/qwen/qwen-v2.md)
* [tuning](tuning/README.md)
  * [\_index](tuning/\_index.md)
  * [DPO](tuning/dpo.md)
  * [Lora](tuning/lora.md)
  * [NEFTune](tuning/neftune.md)
  * [P-Tuning V2](tuning/p-tuning-v2.md)
  * [P-Tuning](tuning/p-tuning.md)
  * [PPO](tuning/ppo.md)
  * [Q-Lora](tuning/q-lora.md)
  * [MOE-Lora](tuning/moe-lora.md)
* [ðŸ¤” hullusion-and-training-innovations](hullusion-and-training-innovations/README.md)
  * [\_index](hullusion-and-training-innovations/\_index.md)
  * [LM-Infinite](hullusion-and-training-innovations/lm-infinite.md)
* [knowledge-injection](knowledge-injection/README.md)
  * [\_index](knowledge-injection/\_index.md)
  * [Can LMs Learn New Entities from Descriptions?](knowledge-injection/can-lms-learn-new-entities-from-descriptions.md)
* [TimeSeries Prediction of LLM](timeseries-prediction-of-llm/README.md)
  * [TS-for-LLM](timeseries-prediction-of-llm/ts-for-llm/README.md)
    * [One Fits All: Power General Time Series Analysis by Pretrained LM](timeseries-prediction-of-llm/ts-for-llm/one-fits-all-power-general-time-series-analysis-by-pretrained-lm.md)
    * [LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs](timeseries-prediction-of-llm/ts-for-llm/llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms.md)
    * [LLM for Time Seriesï¼šText Prototype Aligned Embedding to Activate LLMâ€™s Ability for Time Series](timeseries-prediction-of-llm/ts-for-llm/llm-for-time-series-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series.md)
    * [TEMPO: prompt-based generative pre-trained transformer for time series forecasting](timeseries-prediction-of-llm/ts-for-llm/tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting.md)
    * [Large Language Models Are Zero-Shot Time Series Forecasters \[NeurIPS2023\]](timeseries-prediction-of-llm/ts-for-llm/large-language-models-are-zero-shot-time-series-forecasters-neurips2023.md)
* [other-papers](other-papers/README.md)
  * [\_index](other-papers/\_index.md)
  * [Flan](other-papers/flan.md)
  * [Where to go Next for Recommendation Systems?](other-papers/where-to-go-next-for-recommendation-systems.md)
  * [TabLLM](other-papers/tabllm/README.md)
    * [\_index](other-papers/tabllm/\_index.md)
    * [Tree-based models won in tabular](other-papers/tabllm/tree-based-models-won-in-tabular.md)
