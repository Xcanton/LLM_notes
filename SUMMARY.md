# Table of contents

* [ðŸŒŒ before-llm-era](README.md)
  * [Pre-trained Models](before-llm-era/pre-trained-models/README.md)
    * [Bert](before-llm-era/pre-trained-models/bert.md)
    * [Transformer](before-llm-era/pre-trained-models/transformer.md)
    * [Attention](before-llm-era/pre-trained-models/attention/README.md)
      * [Multi-head Attention](before-llm-era/pre-trained-models/attention/multi-head-attention.md)
      * [Self-Attention](before-llm-era/pre-trained-models/attention/self-attention.md)
    * [gpt](before-llm-era/pre-trained-models/gpt.md)
  * [Word Embeddings](before-llm-era/word-embeddings/README.md)
    * [NNLM](before-llm-era/word-embeddings/nnlm.md)
    * [Word2Vec](before-llm-era/word-embeddings/word2vec.md)
    * [Glove](before-llm-era/word-embeddings/glove.md)
    * [ELMO](before-llm-era/word-embeddings/elmo.md)
* [ðŸŒ… the-dawn-of-llm-era](the-dawn-of-llm-era/README.md)
  * [Bloom](the-dawn-of-llm-era/bloom.md)
  * [Other LLMs](the-dawn-of-llm-era/other-llms.md)
  * [T5](the-dawn-of-llm-era/t5.md)
  * [Baichuan](the-dawn-of-llm-era/baichuan/README.md)
    * [Baichuan-v2](the-dawn-of-llm-era/baichuan/baichuan-v2.md)
  * [ChatGLM](the-dawn-of-llm-era/chatglm/README.md)
    * [ChatGLM-v2](the-dawn-of-llm-era/chatglm/chatglm-v2.md)
    * [ChatGLM-v3](the-dawn-of-llm-era/chatglm/chatglm-v3.md)
    * [GLM](the-dawn-of-llm-era/chatglm/glm.md)
  * [GPT Series](the-dawn-of-llm-era/gpt-series/README.md)
    * [GPT2](the-dawn-of-llm-era/gpt-series/gpt2.md)
    * [Instruction-GPT](the-dawn-of-llm-era/gpt-series/instruction-gpt.md)
    * [GPT4](the-dawn-of-llm-era/gpt-series/gpt4/README.md)
      * [GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE](the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe.md)
  * [LLaMa Series](the-dawn-of-llm-era/llama-series/README.md)
    * [LLaMa](the-dawn-of-llm-era/llama-series/llama.md)
  * [P5](the-dawn-of-llm-era/p5/README.md)
    * [How to Index Item IDs](the-dawn-of-llm-era/p5/how-to-index-item-ids.md)
  * [Qwen](the-dawn-of-llm-era/qwen/README.md)
    * [\_index](the-dawn-of-llm-era/qwen/\_index.md)
    * [Qwen-v2](the-dawn-of-llm-era/qwen/qwen-v2.md)
  * [Flan](the-dawn-of-llm-era/flan.md)
* [tuning](tuning/README.md)
  * [DPO](tuning/dpo.md)
  * [Lora](tuning/lora/README.md)
    * [Experience1 Effectiveness](tuning/lora/experience1-effectiveness.md)
    * [Experience2 Hyper-parameters](tuning/lora/experience2-hyper-parameters.md)
    * [Experience3 Mechanism](tuning/lora/experience3-mechanism.md)
  * [NEFTune](tuning/neftune.md)
  * [P-Tuning V2](tuning/p-tuning-v2.md)
  * [P-Tuning](tuning/p-tuning.md)
  * [PPO](tuning/ppo.md)
  * [Q-Lora](tuning/q-lora.md)
  * [MOE-Lora](tuning/moe-lora.md)
* [ðŸ¤” hullusion-and-training-innovations](hullusion-and-training-innovations/README.md)
  * [LM-Infinite](hullusion-and-training-innovations/lm-infinite.md)
* [knowledge-injection](knowledge-injection/README.md)
  * [Can LMs Learn New Entities from Descriptions?](knowledge-injection/can-lms-learn-new-entities-from-descriptions.md)
* [TimeSeries Prediction of LLM](timeseries-prediction-of-llm/README.md)
  * [TS-for-LLM](timeseries-prediction-of-llm/ts-for-llm/README.md)
    * [One Fits All: Power General Time Series Analysis by Pretrained LM](timeseries-prediction-of-llm/ts-for-llm/one-fits-all-power-general-time-series-analysis-by-pretrained-lm.md)
    * [LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs](timeseries-prediction-of-llm/ts-for-llm/llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms.md)
    * [LLM for Time Seriesï¼šText Prototype Aligned Embedding to Activate LLMâ€™s Ability for Time Series](timeseries-prediction-of-llm/ts-for-llm/llm-for-time-series-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series.md)
    * [TEMPO: prompt-based generative pre-trained transformer for time series forecasting](timeseries-prediction-of-llm/ts-for-llm/tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting.md)
    * [Large Language Models Are Zero-Shot Time Series Forecasters \[NeurIPS2023\]](timeseries-prediction-of-llm/ts-for-llm/large-language-models-are-zero-shot-time-series-forecasters-neurips2023.md)
    * [Time-LLM: Time Series Forecasting by Reprogramming Large Language Models \[ICLR 2024\]](timeseries-prediction-of-llm/ts-for-llm/time-llm-time-series-forecasting-by-reprogramming-large-language-models-iclr-2024.md)
* [other-papers](other-papers/README.md)
  * [Where to go Next for Recommendation Systems?](other-papers/where-to-go-next-for-recommendation-systems.md)
  * [TabLLM](other-papers/tabllm/README.md)
    * [Tree-based models won in tabular](other-papers/tabllm/tree-based-models-won-in-tabular.md)
