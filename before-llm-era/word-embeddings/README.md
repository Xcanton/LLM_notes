# Word Embeddings

如何将文本转化为向量表征，其实是一个历史悠久的事情。NLP算法工程师们花了大量的时间研究如何将词向量表征实现得更好。

## [nnlm.md](nnlm.md "mention")

最早期的工作可以追溯到2003年的《A Neural Probabilistic Language Model》。这篇文章发表于2003年，但是火于2013年，即在深度学习深入自然语言领域的大火之年，被从积灰的角落抽出这个压箱底的文章，送上了王座。这篇文章创新性地提出用Next-Item-Prediction的目标建模，并且借鉴统计方法，对文本进行表征。大概的公式如下：

$$
y=b+Wx+U \tanh {(d+Hx)} \\ x=\{c_1, c_2, \cdots, c_n\}
$$

这篇文章通过词向量的方式，将one-hot编码映射到低维空间的稠密向量表征。再通过深度网络特征提取和线性判别层对下一词的概率进行判别。这样在反向传播的过程中，词向量可以考虑到局部句子结构，得到较好的词向量产物。

该论文算是Word Embedding的革命第一枪，发出时不火的原因，只能说它在大家都用统计办法（隐马尔可夫等）的时候提出了超前的方式，并不被大家所理解罢了。
