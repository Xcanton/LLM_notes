# TimeSeries Prediction of LLM

目前大语言模型在时间序列预测的研究主要分成了两个方向:

* 一个是**LLM -for-TS**: 从零开始设计并预训练适用于时间序列的基础大模型, 然后可根据各种下游任务对模型进行微调。这条路径是最基本的解决方案，基于大量数据，通过预训练向模型灌输时间序列相关知识。但时间序列数据更专业且涉及隐私问题，获取大量的时间序列数据困难， 而且由于不同领域的时间序列数据存在重大差异，需要从头开始构建和训练针对不同垂域的各种模型；
* 一个是 **TS-for-LLM:** 设计相应机制对时间序列输入大模型进行适配，使其能够适用于现有的语言模型，从而基于现有的语言模型处理时间序列的各类任务。这条路径也具有一定的挑战性，需要超越原始语言模型的能力，补充时间序列语意信息。

两种方向的区别是：

* 一种比较自然的方法是将时间序列当成文本序列，但对处理多变量时间序列比较敏感；
* 第二种是对时间序列进行tokenize，设计一个模块编码TS tokens,并取代原有LLM的embedding layer， 其核心是创建LLM能够理解的ts embedding。
